{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTDV5hqRwsOy"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i in range(1,1401):\n",
        "    stri = \"cranfield\"\n",
        "    temp = str(i)\n",
        "    leng = len(temp)\n",
        "    req = 4 - leng\n",
        "    stri += \"0\" * req \n",
        "    stri += temp\n",
        "    # stri += \".txt\"\n",
        "    file = open(stri , \"r\")\n",
        "    text = file.read()\n",
        "    data.append(text)\n",
        "    stri = \"cranfield\""
      ],
      "metadata": {
        "id": "09W1n20Uw5H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "7T5Zqsv86774",
        "outputId": "15dbf7d5-938e-451c-d7af-a03de5937333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<DOC>\\n<DOCNO>\\n1\\n</DOCNO>\\n<TITLE>\\nexperimental investigation of the aerodynamics of a\\nwing in a slipstream .\\n</TITLE>\\n<AUTHOR>\\nbrenckman,m.\\n</AUTHOR>\\n<BIBLIO>\\nj. ae. scs. 25, 1958, 324.\\n</BIBLIO>\\n<TEXT>\\n  an experimental study of a wing in a propeller slipstream was\\nmade in order to determine the spanwise distribution of the lift\\nincrease due to slipstream at different angles of attack of the wing\\nand at different free stream to slipstream velocity ratios .  the\\nresults were intended in part as an evaluation basis for different\\ntheoretical treatments of this problem .\\n  the comparative span loading curves, together with supporting\\nevidence, showed that a substantial part of the lift increment\\nproduced by the slipstream was due to a /destalling/ or boundary-layer-control\\neffect .  the integrated remaining lift increment,\\nafter subtracting this destalling lift, was found to agree\\nwell with a potential flow theory .\\n  an empirical evaluation of the destalling effects was made for\\nthe specific configuration of the experiment .\\n</TEXT>\\n</DOC>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = np.array(data)\n",
        "pd.DataFrame(temp).to_csv('data.csv') \n",
        "files.download(\"data.csv\")"
      ],
      "metadata": {
        "id": "9zlsMvaO7k63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step # 1 : Extracting Data from each file's text"
      ],
      "metadata": {
        "id": "R9jAnauN17iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ab1QwvEf1-pq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "5cbe74ae-f04e-4a4a-f51d-60c87bbdb5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d51e8bf4-34a6-4d1a-9495-e4cfe9f2d303\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d51e8bf4-34a6-4d1a-9495-e4cfe9f2d303\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.csv to data (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GbDmVzag2QHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"data.csv\")"
      ],
      "metadata": {
        "id": "HW9tOnZT2jqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main = []"
      ],
      "metadata": {
        "id": "p3eyoTmHvEeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data)):\n",
        "    txt = data.iloc[i][1]\n",
        "\n",
        "    store = \"\"\n",
        "\n",
        "    # first we will find location of \"<TITLE>\"\n",
        "    \n",
        "    ii = 0\n",
        "\n",
        "    for t in range(len(txt)):\n",
        "        if (txt[t] == '<' and txt[t+1] == 'T' and txt[t+2] == 'I' and txt[t+3] == 'T' and txt[t+4] == 'L' and txt[t+5] == 'E' and txt[t+6] == '>'):\n",
        "            ii = t\n",
        "            break\n",
        "    \n",
        "    ii += 7\n",
        "\n",
        "    # print(\"check \" , ii)\n",
        "\n",
        "    while(ii < len(txt)):\n",
        "        if(txt[ii] == '\\n' and txt[ii+1] == '<' and txt[ii+2] == '/' and  txt[ii+3] == 'T' and txt[ii+4] == 'I' and txt[ii+5] == 'T' and txt[ii+6] == 'L' and txt[ii+7] == 'E' and txt[ii+8] == '>' ):\n",
        "            break\n",
        "        if(txt[ii] == \"\\n\"):\n",
        "            ii += 1\n",
        "            continue\n",
        "        store += txt[ii]\n",
        "        ii += 1\n",
        "    \n",
        "    store += \" \"\n",
        "\n",
        "    # Now we will find <TEXT>\n",
        "    for t in range(len(txt)):\n",
        "        if (txt[t] == '<' and txt[t+1] == 'T' and txt[t+2] == 'E' and txt[t+3] == 'X' and txt[t+4] == 'T' and txt[t+5] == '>'):\n",
        "            ii = t\n",
        "            break\n",
        "    \n",
        "    ii += 6\n",
        "\n",
        "    while(ii < len(txt)):\n",
        "\n",
        "        if(txt[ii] == \"\\n\" and txt[ii+1] == \"<\" and txt[ii+2] == '/' and  txt[ii+3] == 'T' and txt[ii+4] == 'E' and txt[ii+5] == 'X' and txt[ii+6] == 'T' and txt[ii+7] == '>' ):\n",
        "            break\n",
        "        if(txt[ii] == \"\\n\"):\n",
        "            ii += 1\n",
        "            continue\n",
        "        store += txt[ii]\n",
        "        ii += 1\n",
        "\n",
        "    main.append(store)\n"
      ],
      "metadata": {
        "id": "lHbG5MnN2u1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g9Jc9ILs6rJ_",
        "outputId": "7600a522-cb83-485f-83e7-ad5d76973bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the boundary layer in simple shear flow past a flat plate . the boundary-layer equations are presented for steadyincompressible flow with no pressure gradient .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing data\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "metadata": {
        "id": "BDvCmy8Z9J_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad16bc7-c6f5-46e4-8df9-d1a65f275991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pre_process_data(main , flag = False):\n",
        "\n",
        "    if (flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            Original Data\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" , main[0])\n",
        "        print(\"2 :\" ,main[1])\n",
        "        print(\"3 :\" ,main[2])\n",
        "        print(\"4 :\" ,main[3])\n",
        "        print(\"5 :\" ,main[4])\n",
        "\n",
        "\n",
        "    # Step 1 - lowercasing\n",
        "\n",
        "    lower_cased_data = []\n",
        "    for sent in main:\n",
        "        lower_cased_data.append(sent.lower())\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Lower Casing\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,lower_cased_data[0])\n",
        "        print(\"2 :\" ,lower_cased_data[1])\n",
        "        print(\"3 :\" ,lower_cased_data[2])\n",
        "        print(\"4 :\" ,lower_cased_data[3])\n",
        "        print(\"5 :\" ,lower_cased_data[4])\n",
        "\n",
        "    # Step 2 - Tokenization\n",
        "\n",
        "    token = WhitespaceTokenizer()\n",
        "\n",
        "    tokenized_data = []\n",
        "    for sent in lower_cased_data:\n",
        "        tokenized_data.append(token.tokenize(sent))\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Toeknization\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,tokenized_data[0])\n",
        "        print(\"2 :\" ,tokenized_data[1])\n",
        "        print(\"3 :\" ,tokenized_data[2])\n",
        "        print(\"4 :\" ,tokenized_data[3])\n",
        "        print(\"5 :\" ,tokenized_data[4])\n",
        "\n",
        "    \n",
        "    # Step 3 - removing stop words\n",
        "\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    no_stop_data = []\n",
        "\n",
        "    for li in tokenized_data:\n",
        "        temp = []\n",
        "        for word in li:\n",
        "            if (word not in stopWords):\n",
        "                temp.append(word)\n",
        "        no_stop_data.append(temp)\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Stop Words removal\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,no_stop_data[0])\n",
        "        print(\"2 :\" ,no_stop_data[1])\n",
        "        print(\"3 :\" ,no_stop_data[2])\n",
        "        print(\"4 :\" ,no_stop_data[3])\n",
        "        print(\"5 :\" ,no_stop_data[4])\n",
        "\n",
        "    # Step 4 - removing punctuations\n",
        "\n",
        "    puncts = string.punctuation\n",
        "    no_punc_data = []\n",
        "\n",
        "    for li in no_stop_data:\n",
        "        temp = []\n",
        "        for word in li:\n",
        "            temp_word = \"\"\n",
        "            for alp in word:\n",
        "                if (alp not in puncts):\n",
        "                    temp_word += alp\n",
        "            temp.append(temp_word)\n",
        "        no_punc_data.append(temp)\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After Punctuations removal\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,no_punc_data[0])\n",
        "        print(\"2 :\" ,no_punc_data[1])\n",
        "        print(\"3 :\" ,no_punc_data[2])\n",
        "        print(\"4 :\" ,no_punc_data[3])\n",
        "        print(\"5 :\" ,no_punc_data[4])\n",
        "\n",
        "    # Step 5 - Removing white spaces\n",
        "\n",
        "    final_data = []\n",
        "\n",
        "    for li in no_punc_data:\n",
        "        temp = []\n",
        "        for word in li:\n",
        "            if (word != ''):\n",
        "                temp.append(word)\n",
        "        final_data.append(temp)\n",
        "\n",
        "    if(flag):\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print (\"            After White Space removal\")\n",
        "        print(\"-----------------------------------------------\")\n",
        "        print(\"1 :\" ,final_data[0])\n",
        "        print(\"2 :\" ,final_data[1])\n",
        "        print(\"3 :\" ,final_data[2])\n",
        "        print(\"4 :\" ,final_data[3])\n",
        "        print(\"5 :\" ,final_data[4])\n",
        "\n",
        "    return final_data"
      ],
      "metadata": {
        "id": "_Ey_dWcMJ8R-"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uJ71brcNkpD",
        "outputId": "f46d03e6-207b-4cb2-e596-31007c05a3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = pre_process_data(main , True)"
      ],
      "metadata": {
        "id": "uMJNCiVTLnwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971e641b-7882-4de2-d551-0c49bc1e9435"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "            Original Data\n",
            "-----------------------------------------------\n",
            "1 : experimental investigation of the aerodynamics of awing in a slipstream .   an experimental study of a wing in a propeller slipstream wasmade in order to determine the spanwise distribution of the liftincrease due to slipstream at different angles of attack of the wingand at different free stream to slipstream velocity ratios .  theresults were intended in part as an evaluation basis for differenttheoretical treatments of this problem .  the comparative span loading curves, together with supportingevidence, showed that a substantial part of the lift incrementproduced by the slipstream was due to a /destalling/ or boundary-layer-controleffect .  the integrated remaining lift increment,after subtracting this destalling lift, was found to agreewell with a potential flow theory .  an empirical evaluation of the destalling effects was made forthe specific configuration of the experiment .\n",
            "2 : simple shear flow past a flat plate in an incompressible fluid of smallviscosity . in the study of high-speed viscous flow past a two-dimensional body itis usually necessary to consider a curved shock wave emitting from thenose or leading edge of the body .  consequently, there exists an inviscidrotational flow region between the shock wave and the boundary layer.  such a situation arises, for instance, in the study of the hypersonicviscous flow past a flat plate .  the situation is somewhat differentfrom prandtl's classical boundary-layer problem . in prandtl'soriginal problem the inviscid free stream outside the boundary layer isirrotational while in a hypersonic boundary-layer problem the inviscidfree stream must be considered as rotational .  the possible effects ofvorticity have been recently discussed by ferri and libby .  in the presentpaper, the simple shear flow past a flat plate in a fluid of smallviscosity is investigated .  it can be shown that this problem can againbe treated by the boundary-layer approximation, the only novel featurebeing that the free stream has a constant vorticity .  the discussionhere is restricted to two-dimensional incompressible steady flow .\n",
            "3 : the boundary layer in simple shear flow past a flat plate . the boundary-layer equations are presented for steadyincompressible flow with no pressure gradient .\n",
            "4 : approximate solutions of the incompressible laminarboundary layer equations for a plate in shear flow .   the two-dimensional steady boundary-layerproblem for a flat plate in ashear flow of incompressible fluid is considered .solutions for the boundarylayerthickness, skin friction, and the velocitydistribution in the boundarylayer are obtained by the karman-pohlhausentechnique .  comparison withthe boundary layer of a uniform flow has alsobeen made to show the effect ofvorticity .\n",
            "5 : one-dimensional transient heat conduction into a double-layerslab subjected to a linear heat input for a small timeinternal .   analytic solutions are presented for the transient heat conductionin composite slabs exposed at one surface to atriangular heat rate .  this type of heating rate may occur, forexample, during aerodynamic heating .\n",
            "-----------------------------------------------\n",
            "            After Lower Casing\n",
            "-----------------------------------------------\n",
            "1 : experimental investigation of the aerodynamics of awing in a slipstream .   an experimental study of a wing in a propeller slipstream wasmade in order to determine the spanwise distribution of the liftincrease due to slipstream at different angles of attack of the wingand at different free stream to slipstream velocity ratios .  theresults were intended in part as an evaluation basis for differenttheoretical treatments of this problem .  the comparative span loading curves, together with supportingevidence, showed that a substantial part of the lift incrementproduced by the slipstream was due to a /destalling/ or boundary-layer-controleffect .  the integrated remaining lift increment,after subtracting this destalling lift, was found to agreewell with a potential flow theory .  an empirical evaluation of the destalling effects was made forthe specific configuration of the experiment .\n",
            "2 : simple shear flow past a flat plate in an incompressible fluid of smallviscosity . in the study of high-speed viscous flow past a two-dimensional body itis usually necessary to consider a curved shock wave emitting from thenose or leading edge of the body .  consequently, there exists an inviscidrotational flow region between the shock wave and the boundary layer.  such a situation arises, for instance, in the study of the hypersonicviscous flow past a flat plate .  the situation is somewhat differentfrom prandtl's classical boundary-layer problem . in prandtl'soriginal problem the inviscid free stream outside the boundary layer isirrotational while in a hypersonic boundary-layer problem the inviscidfree stream must be considered as rotational .  the possible effects ofvorticity have been recently discussed by ferri and libby .  in the presentpaper, the simple shear flow past a flat plate in a fluid of smallviscosity is investigated .  it can be shown that this problem can againbe treated by the boundary-layer approximation, the only novel featurebeing that the free stream has a constant vorticity .  the discussionhere is restricted to two-dimensional incompressible steady flow .\n",
            "3 : the boundary layer in simple shear flow past a flat plate . the boundary-layer equations are presented for steadyincompressible flow with no pressure gradient .\n",
            "4 : approximate solutions of the incompressible laminarboundary layer equations for a plate in shear flow .   the two-dimensional steady boundary-layerproblem for a flat plate in ashear flow of incompressible fluid is considered .solutions for the boundarylayerthickness, skin friction, and the velocitydistribution in the boundarylayer are obtained by the karman-pohlhausentechnique .  comparison withthe boundary layer of a uniform flow has alsobeen made to show the effect ofvorticity .\n",
            "5 : one-dimensional transient heat conduction into a double-layerslab subjected to a linear heat input for a small timeinternal .   analytic solutions are presented for the transient heat conductionin composite slabs exposed at one surface to atriangular heat rate .  this type of heating rate may occur, forexample, during aerodynamic heating .\n",
            "-----------------------------------------------\n",
            "            After Toeknization\n",
            "-----------------------------------------------\n",
            "1 : ['experimental', 'investigation', 'of', 'the', 'aerodynamics', 'of', 'awing', 'in', 'a', 'slipstream', '.', 'an', 'experimental', 'study', 'of', 'a', 'wing', 'in', 'a', 'propeller', 'slipstream', 'wasmade', 'in', 'order', 'to', 'determine', 'the', 'spanwise', 'distribution', 'of', 'the', 'liftincrease', 'due', 'to', 'slipstream', 'at', 'different', 'angles', 'of', 'attack', 'of', 'the', 'wingand', 'at', 'different', 'free', 'stream', 'to', 'slipstream', 'velocity', 'ratios', '.', 'theresults', 'were', 'intended', 'in', 'part', 'as', 'an', 'evaluation', 'basis', 'for', 'differenttheoretical', 'treatments', 'of', 'this', 'problem', '.', 'the', 'comparative', 'span', 'loading', 'curves,', 'together', 'with', 'supportingevidence,', 'showed', 'that', 'a', 'substantial', 'part', 'of', 'the', 'lift', 'incrementproduced', 'by', 'the', 'slipstream', 'was', 'due', 'to', 'a', '/destalling/', 'or', 'boundary-layer-controleffect', '.', 'the', 'integrated', 'remaining', 'lift', 'increment,after', 'subtracting', 'this', 'destalling', 'lift,', 'was', 'found', 'to', 'agreewell', 'with', 'a', 'potential', 'flow', 'theory', '.', 'an', 'empirical', 'evaluation', 'of', 'the', 'destalling', 'effects', 'was', 'made', 'forthe', 'specific', 'configuration', 'of', 'the', 'experiment', '.']\n",
            "2 : ['simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'an', 'incompressible', 'fluid', 'of', 'smallviscosity', '.', 'in', 'the', 'study', 'of', 'high-speed', 'viscous', 'flow', 'past', 'a', 'two-dimensional', 'body', 'itis', 'usually', 'necessary', 'to', 'consider', 'a', 'curved', 'shock', 'wave', 'emitting', 'from', 'thenose', 'or', 'leading', 'edge', 'of', 'the', 'body', '.', 'consequently,', 'there', 'exists', 'an', 'inviscidrotational', 'flow', 'region', 'between', 'the', 'shock', 'wave', 'and', 'the', 'boundary', 'layer.', 'such', 'a', 'situation', 'arises,', 'for', 'instance,', 'in', 'the', 'study', 'of', 'the', 'hypersonicviscous', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'situation', 'is', 'somewhat', 'differentfrom', \"prandtl's\", 'classical', 'boundary-layer', 'problem', '.', 'in', \"prandtl'soriginal\", 'problem', 'the', 'inviscid', 'free', 'stream', 'outside', 'the', 'boundary', 'layer', 'isirrotational', 'while', 'in', 'a', 'hypersonic', 'boundary-layer', 'problem', 'the', 'inviscidfree', 'stream', 'must', 'be', 'considered', 'as', 'rotational', '.', 'the', 'possible', 'effects', 'ofvorticity', 'have', 'been', 'recently', 'discussed', 'by', 'ferri', 'and', 'libby', '.', 'in', 'the', 'presentpaper,', 'the', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'a', 'fluid', 'of', 'smallviscosity', 'is', 'investigated', '.', 'it', 'can', 'be', 'shown', 'that', 'this', 'problem', 'can', 'againbe', 'treated', 'by', 'the', 'boundary-layer', 'approximation,', 'the', 'only', 'novel', 'featurebeing', 'that', 'the', 'free', 'stream', 'has', 'a', 'constant', 'vorticity', '.', 'the', 'discussionhere', 'is', 'restricted', 'to', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "3 : ['the', 'boundary', 'layer', 'in', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'boundary-layer', 'equations', 'are', 'presented', 'for', 'steadyincompressible', 'flow', 'with', 'no', 'pressure', 'gradient', '.']\n",
            "4 : ['approximate', 'solutions', 'of', 'the', 'incompressible', 'laminarboundary', 'layer', 'equations', 'for', 'a', 'plate', 'in', 'shear', 'flow', '.', 'the', 'two-dimensional', 'steady', 'boundary-layerproblem', 'for', 'a', 'flat', 'plate', 'in', 'ashear', 'flow', 'of', 'incompressible', 'fluid', 'is', 'considered', '.solutions', 'for', 'the', 'boundarylayerthickness,', 'skin', 'friction,', 'and', 'the', 'velocitydistribution', 'in', 'the', 'boundarylayer', 'are', 'obtained', 'by', 'the', 'karman-pohlhausentechnique', '.', 'comparison', 'withthe', 'boundary', 'layer', 'of', 'a', 'uniform', 'flow', 'has', 'alsobeen', 'made', 'to', 'show', 'the', 'effect', 'ofvorticity', '.']\n",
            "5 : ['one-dimensional', 'transient', 'heat', 'conduction', 'into', 'a', 'double-layerslab', 'subjected', 'to', 'a', 'linear', 'heat', 'input', 'for', 'a', 'small', 'timeinternal', '.', 'analytic', 'solutions', 'are', 'presented', 'for', 'the', 'transient', 'heat', 'conductionin', 'composite', 'slabs', 'exposed', 'at', 'one', 'surface', 'to', 'atriangular', 'heat', 'rate', '.', 'this', 'type', 'of', 'heating', 'rate', 'may', 'occur,', 'forexample,', 'during', 'aerodynamic', 'heating', '.']\n",
            "-----------------------------------------------\n",
            "            After Stop Words removal\n",
            "-----------------------------------------------\n",
            "1 : ['experimental', 'investigation', 'aerodynamics', 'awing', 'slipstream', '.', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'wasmade', 'order', 'determine', 'spanwise', 'distribution', 'liftincrease', 'due', 'slipstream', 'different', 'angles', 'attack', 'wingand', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '.', 'theresults', 'intended', 'part', 'evaluation', 'basis', 'differenttheoretical', 'treatments', 'problem', '.', 'comparative', 'span', 'loading', 'curves,', 'together', 'supportingevidence,', 'showed', 'substantial', 'part', 'lift', 'incrementproduced', 'slipstream', 'due', '/destalling/', 'boundary-layer-controleffect', '.', 'integrated', 'remaining', 'lift', 'increment,after', 'subtracting', 'destalling', 'lift,', 'found', 'agreewell', 'potential', 'flow', 'theory', '.', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'forthe', 'specific', 'configuration', 'experiment', '.']\n",
            "2 : ['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'smallviscosity', '.', 'study', 'high-speed', 'viscous', 'flow', 'past', 'two-dimensional', 'body', 'itis', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'thenose', 'leading', 'edge', 'body', '.', 'consequently,', 'exists', 'inviscidrotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer.', 'situation', 'arises,', 'instance,', 'study', 'hypersonicviscous', 'flow', 'past', 'flat', 'plate', '.', 'situation', 'somewhat', 'differentfrom', \"prandtl's\", 'classical', 'boundary-layer', 'problem', '.', \"prandtl'soriginal\", 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'isirrotational', 'hypersonic', 'boundary-layer', 'problem', 'inviscidfree', 'stream', 'must', 'considered', 'rotational', '.', 'possible', 'effects', 'ofvorticity', 'recently', 'discussed', 'ferri', 'libby', '.', 'presentpaper,', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'smallviscosity', 'investigated', '.', 'shown', 'problem', 'againbe', 'treated', 'boundary-layer', 'approximation,', 'novel', 'featurebeing', 'free', 'stream', 'constant', 'vorticity', '.', 'discussionhere', 'restricted', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "3 : ['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '.', 'boundary-layer', 'equations', 'presented', 'steadyincompressible', 'flow', 'pressure', 'gradient', '.']\n",
            "4 : ['approximate', 'solutions', 'incompressible', 'laminarboundary', 'layer', 'equations', 'plate', 'shear', 'flow', '.', 'two-dimensional', 'steady', 'boundary-layerproblem', 'flat', 'plate', 'ashear', 'flow', 'incompressible', 'fluid', 'considered', '.solutions', 'boundarylayerthickness,', 'skin', 'friction,', 'velocitydistribution', 'boundarylayer', 'obtained', 'karman-pohlhausentechnique', '.', 'comparison', 'withthe', 'boundary', 'layer', 'uniform', 'flow', 'alsobeen', 'made', 'show', 'effect', 'ofvorticity', '.']\n",
            "5 : ['one-dimensional', 'transient', 'heat', 'conduction', 'double-layerslab', 'subjected', 'linear', 'heat', 'input', 'small', 'timeinternal', '.', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conductionin', 'composite', 'slabs', 'exposed', 'one', 'surface', 'atriangular', 'heat', 'rate', '.', 'type', 'heating', 'rate', 'may', 'occur,', 'forexample,', 'aerodynamic', 'heating', '.']\n",
            "-----------------------------------------------\n",
            "            After Punctuations removal\n",
            "-----------------------------------------------\n",
            "1 : ['experimental', 'investigation', 'aerodynamics', 'awing', 'slipstream', '', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'wasmade', 'order', 'determine', 'spanwise', 'distribution', 'liftincrease', 'due', 'slipstream', 'different', 'angles', 'attack', 'wingand', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '', 'theresults', 'intended', 'part', 'evaluation', 'basis', 'differenttheoretical', 'treatments', 'problem', '', 'comparative', 'span', 'loading', 'curves', 'together', 'supportingevidence', 'showed', 'substantial', 'part', 'lift', 'incrementproduced', 'slipstream', 'due', 'destalling', 'boundarylayercontroleffect', '', 'integrated', 'remaining', 'lift', 'incrementafter', 'subtracting', 'destalling', 'lift', 'found', 'agreewell', 'potential', 'flow', 'theory', '', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'forthe', 'specific', 'configuration', 'experiment', '']\n",
            "2 : ['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'smallviscosity', '', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'itis', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'thenose', 'leading', 'edge', 'body', '', 'consequently', 'exists', 'inviscidrotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonicviscous', 'flow', 'past', 'flat', 'plate', '', 'situation', 'somewhat', 'differentfrom', 'prandtls', 'classical', 'boundarylayer', 'problem', '', 'prandtlsoriginal', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'isirrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscidfree', 'stream', 'must', 'considered', 'rotational', '', 'possible', 'effects', 'ofvorticity', 'recently', 'discussed', 'ferri', 'libby', '', 'presentpaper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'smallviscosity', 'investigated', '', 'shown', 'problem', 'againbe', 'treated', 'boundarylayer', 'approximation', 'novel', 'featurebeing', 'free', 'stream', 'constant', 'vorticity', '', 'discussionhere', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow', '']\n",
            "3 : ['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '', 'boundarylayer', 'equations', 'presented', 'steadyincompressible', 'flow', 'pressure', 'gradient', '']\n",
            "4 : ['approximate', 'solutions', 'incompressible', 'laminarboundary', 'layer', 'equations', 'plate', 'shear', 'flow', '', 'twodimensional', 'steady', 'boundarylayerproblem', 'flat', 'plate', 'ashear', 'flow', 'incompressible', 'fluid', 'considered', 'solutions', 'boundarylayerthickness', 'skin', 'friction', 'velocitydistribution', 'boundarylayer', 'obtained', 'karmanpohlhausentechnique', '', 'comparison', 'withthe', 'boundary', 'layer', 'uniform', 'flow', 'alsobeen', 'made', 'show', 'effect', 'ofvorticity', '']\n",
            "5 : ['onedimensional', 'transient', 'heat', 'conduction', 'doublelayerslab', 'subjected', 'linear', 'heat', 'input', 'small', 'timeinternal', '', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conductionin', 'composite', 'slabs', 'exposed', 'one', 'surface', 'atriangular', 'heat', 'rate', '', 'type', 'heating', 'rate', 'may', 'occur', 'forexample', 'aerodynamic', 'heating', '']\n",
            "-----------------------------------------------\n",
            "            After White Space removal\n",
            "-----------------------------------------------\n",
            "1 : ['experimental', 'investigation', 'aerodynamics', 'awing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'wasmade', 'order', 'determine', 'spanwise', 'distribution', 'liftincrease', 'due', 'slipstream', 'different', 'angles', 'attack', 'wingand', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', 'theresults', 'intended', 'part', 'evaluation', 'basis', 'differenttheoretical', 'treatments', 'problem', 'comparative', 'span', 'loading', 'curves', 'together', 'supportingevidence', 'showed', 'substantial', 'part', 'lift', 'incrementproduced', 'slipstream', 'due', 'destalling', 'boundarylayercontroleffect', 'integrated', 'remaining', 'lift', 'incrementafter', 'subtracting', 'destalling', 'lift', 'found', 'agreewell', 'potential', 'flow', 'theory', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'forthe', 'specific', 'configuration', 'experiment']\n",
            "2 : ['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'smallviscosity', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'itis', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'thenose', 'leading', 'edge', 'body', 'consequently', 'exists', 'inviscidrotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonicviscous', 'flow', 'past', 'flat', 'plate', 'situation', 'somewhat', 'differentfrom', 'prandtls', 'classical', 'boundarylayer', 'problem', 'prandtlsoriginal', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'isirrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscidfree', 'stream', 'must', 'considered', 'rotational', 'possible', 'effects', 'ofvorticity', 'recently', 'discussed', 'ferri', 'libby', 'presentpaper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'smallviscosity', 'investigated', 'shown', 'problem', 'againbe', 'treated', 'boundarylayer', 'approximation', 'novel', 'featurebeing', 'free', 'stream', 'constant', 'vorticity', 'discussionhere', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow']\n",
            "3 : ['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'boundarylayer', 'equations', 'presented', 'steadyincompressible', 'flow', 'pressure', 'gradient']\n",
            "4 : ['approximate', 'solutions', 'incompressible', 'laminarboundary', 'layer', 'equations', 'plate', 'shear', 'flow', 'twodimensional', 'steady', 'boundarylayerproblem', 'flat', 'plate', 'ashear', 'flow', 'incompressible', 'fluid', 'considered', 'solutions', 'boundarylayerthickness', 'skin', 'friction', 'velocitydistribution', 'boundarylayer', 'obtained', 'karmanpohlhausentechnique', 'comparison', 'withthe', 'boundary', 'layer', 'uniform', 'flow', 'alsobeen', 'made', 'show', 'effect', 'ofvorticity']\n",
            "5 : ['onedimensional', 'transient', 'heat', 'conduction', 'doublelayerslab', 'subjected', 'linear', 'heat', 'input', 'small', 'timeinternal', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conductionin', 'composite', 'slabs', 'exposed', 'one', 'surface', 'atriangular', 'heat', 'rate', 'type', 'heating', 'rate', 'may', 'occur', 'forexample', 'aerodynamic', 'heating']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ZEymQoh6xs31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DELETE \n",
        "\n",
        "temp = np.array(final_data)\n",
        "pd.DataFrame(temp).to_csv('final_data.csv') \n",
        "files.download(\"final_data.csv\")"
      ],
      "metadata": {
        "id": "oEwA4kM1yjzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating positional-encodings\n",
        "# enco = [doc_freq , { doc_id , [word_freq , list_of_docs]}]\n",
        "# {\"hello\" : [5, [ {3 : [3, [120, 125, 278]]}, {5 : [1, [28] ] }, {10 : [2, [132, 182]]}, {23 : [3, [0, 12, 28]]}, {27 : [1, [2]]} ] }\n",
        "\n",
        "def pos_encod(final_data):\n",
        "\n",
        "    encodings = {}\n",
        "    m = len(final_data)\n",
        "\n",
        "    for i in range(m):\n",
        "        pipe = final_data[i]\n",
        "        for j in range(len(pipe)):\n",
        "            word = pipe[j]\n",
        "            # First we will check that is the word already encountered before or not\n",
        "\n",
        "            if (word not in encodings.keys()):\n",
        "                # The word is not yet encountered\n",
        "\n",
        "                li =[1 , [{i:[1,[j]]}]]\n",
        "                encodings[word] = li\n",
        "\n",
        "                # We have successfully added the dictionary for new word to our encoding\n",
        "            else:\n",
        "                # The word was already encountered before\n",
        "\n",
        "                # Now I will check that has this word been encountered in this document or not\n",
        "                li = encodings[word]\n",
        "                docs = li[1] # docs is list of dicts\n",
        "\n",
        "                flag = False # will record the presence of the word in this doc \n",
        "\n",
        "                for particular_dict in docs:\n",
        "                    doc_id = list(particular_dict.keys())[0]\n",
        "                    if(doc_id == i): \n",
        "                        # the word was present in the same document before\n",
        "                        flag = True\n",
        "                        positional_info = particular_dict[doc_id]\n",
        "                        num = positional_info[0]\n",
        "                        posits = positional_info[1]\n",
        "                        posits.append(j)\n",
        "                        new_info = [num + 1 , posits]\n",
        "\n",
        "                        particular_dict[doc_id] = new_info\n",
        "                if (flag == False):\n",
        "                    # The word was not encountered in the same document before\n",
        "\n",
        "                    num_of_docs = encodings[word][0]\n",
        "                    info = encodings[word][1]\n",
        "\n",
        "                    pos_info = [1,[j]]\n",
        "                    new_dict = {i : pos_info}\n",
        "\n",
        "                    info.append(new_dict)\n",
        "                    num_of_docs += 1\n",
        "\n",
        "                    encodings[word] = [num_of_docs , info]\n",
        "\n",
        "    final_encodings = {}\n",
        "    for key in encodings:\n",
        "        old_encodings = encodings[key]\n",
        "        doc_freq = old_encodings[0]\n",
        "        list_of_dics = old_encodings[1]\n",
        "\n",
        "        doc_dict = {}\n",
        "        for dic in list_of_dics:\n",
        "            doc_dict[list(dic.keys())[0]] = dic[list(dic.keys())[0]]\n",
        "\n",
        "        new_encodings = [doc_freq , doc_dict]\n",
        "\n",
        "        final_encodings[key] = new_encodings\n",
        "    return final_encodings\n",
        "\n",
        "# Unigram encodings\n",
        "\n",
        "def unigram_index(data):\n",
        "  index = {}\n",
        "  for id in range(1400):\n",
        "    tokens = data[id]\n",
        "    for token in tokens:\n",
        "      if token in index.keys():\n",
        "        if(index[token][1][len(index[token][1])-1] == id):\n",
        "           continue\n",
        "        index[token].append(id)\n",
        "        doc_freq = index[token][0]\n",
        "        doc_list = index[token][1]\n",
        "        doc_freq += 1\n",
        "        doc_list.append(id)\n",
        "        index[token] = [doc_freq,doc_list] \n",
        "      else:\n",
        "        index[token] = [1,[1]]\n",
        "  return index\n",
        "\n",
        "# Bigram index\n",
        "\n",
        "def bigram_index(data_list):\n",
        "    bigram_index = {}\n",
        "    for i in range(len(data_list)):\n",
        "        for j in range(len(data_list[i])-1):\n",
        "            bigram = data_list[i][j] + data_list[i][j+1]\n",
        "            if bigram_index.get(bigram) is not None:\n",
        "                bigram_index[bigram][0] += 1\n",
        "                bigram_index[bigram][1].append(i)\n",
        "            else:\n",
        "                bigram_index[bigram] = [1, [i]]\n",
        "    return bigram_index"
      ],
      "metadata": {
        "id": "hJ_l0axe2Kt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "#positional\n",
        "with open('positional_index.pickle', 'wb') as p:\n",
        "    pickle.dump(positional_encodings, p)\n",
        "with open('positional_index.pickle', 'rb') as p:\n",
        "    positional_encodings = pickle.load(p)\n",
        "\n",
        "#bigram\n",
        "with open('bigram_index.pickle', 'wb') as b:\n",
        "    pickle.dump(bigram_encodings, b)\n",
        "with open('bigram_index.pickle', 'rb') as b:\n",
        "    bigram_encodings = pickle.load(b)\n",
        "\n",
        "#unigram\n",
        "with open('unigram_index.pickle', 'wb') as u:\n",
        "    pickle.dump(unigram_encodings, u)\n",
        "with open('unigram_index.pickle', 'rb') as u:\n",
        "    unigram_encodings = pickle.load(u)"
      ],
      "metadata": {
        "id": "D4oZKWyX7Sz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encodings = pos_encod(final_data)\n",
        "unigram_encodings = unigram_index(final_data)\n",
        "bigram_encodings = bigram_index(final_data)"
      ],
      "metadata": {
        "id": "xFO2LTQOBMIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing back to files.\n",
        "\n",
        "for i in range(1400):\n",
        "    line = final_data[i]\n",
        "    file_name = \"file\"\n",
        "    file_name += str(i)\n",
        "    file_name += \".txt\"\n",
        "    file = open(file_name, \"w+\")\n",
        "    content = str(line)\n",
        "    file.write(content)\n",
        "    file.close()\n",
        "    files.download(file_name)"
      ],
      "metadata": {
        "id": "-VjF9er8rbdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing \"AND\" queries\n",
        "\n",
        "def AND(li1 , li2):\n",
        "\n",
        "    out = []\n",
        "    comparisions = 0\n",
        "\n",
        "    n = li1[0]\n",
        "    m = li2[0]\n",
        "\n",
        "    l1 = li1[1]\n",
        "    l1.sort()\n",
        "    l2 = li2[1]\n",
        "    l2.sort()\n",
        "\n",
        "    # Now I will run a two pointer algorithm to compute AND\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while(1):\n",
        "        if(i >= n or j >= m):\n",
        "            break\n",
        "        comparisions += 1\n",
        "        if (l1[i] > l2[j]):\n",
        "            j += 1\n",
        "        elif (l1[i] < l2[j]):\n",
        "            i += 1\n",
        "        else:\n",
        "            out.append(l1[i])\n",
        "            i += 1\n",
        "            j += 1\n",
        "    return (out, comparisions)\n"
      ],
      "metadata": {
        "id": "GekJ3IcxAlhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing \"AND_NOT\" queries\n",
        "\n",
        "def AND_not(li1 , li2):\n",
        "\n",
        "    out = []\n",
        "    comparisions = 0\n",
        "\n",
        "    n = li1[0]\n",
        "    m = li2[0]\n",
        "\n",
        "    l1 = li1[1]\n",
        "    l1.sort()\n",
        "    l2 = li2[1]\n",
        "    l2.sort()\n",
        "\n",
        "    # Now I will run a two pointer algorithm to compute AND\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while(1):\n",
        "        if(i >= n or j >= m):\n",
        "            break\n",
        "        comparisions += 1\n",
        "        if (l1[i] == l2[j]):\n",
        "            i += 1\n",
        "            j += 1\n",
        "        else:\n",
        "            if(l1[i] > l2[j]):\n",
        "                j += 1\n",
        "            else:\n",
        "                out.append(l1[i])\n",
        "                i += 1\n",
        "    return (out, comparisions)\n"
      ],
      "metadata": {
        "id": "qzsz5Xi5GC-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing \"OR\" queries\n",
        "\n",
        "def xORy(docIDs1, docIDs2):\n",
        "  wordlist1 = docIDs1[1]\n",
        "  wordlist2 = docIDs2[1]\n",
        "  m = docIDs1[0]\n",
        "  n = docIDs2[0]\n",
        "  wordlist1.sort()\n",
        "  wordlist2.sort()\n",
        "  p1 = 0\n",
        "  p2 = 0\n",
        "  comparisons = 0\n",
        "  docIDs = []\n",
        "  while p1<m and p2<n:\n",
        "      comparisons += 1\n",
        "      if wordlist1[p1] == wordlist2[p2]:\n",
        "          docIDs.append(wordlist1[p1])\n",
        "          p1 += 1\n",
        "          p2 += 1\n",
        "      else:\n",
        "          if wordlist1[p1] < wordlist2[p2]:\n",
        "              docIDs.append(wordlist1[p1])\n",
        "              p1 += 1\n",
        "          else:\n",
        "              docIDs.append(wordlist2[p2])\n",
        "              p2 += 1\n",
        "\n",
        "  while p1<m:\n",
        "      docIDs.append(wordlist1[p1])\n",
        "      p1 += 1\n",
        "  while p2<n:\n",
        "      docIDs.append(wordlist2[p2])\n",
        "      p2 += 1\n",
        "\n",
        "  return (docIDs, comparisons)"
      ],
      "metadata": {
        "id": "AqU59IcqGqip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xOR_NOTy(docIDs1, docIDs2):\n",
        "    wordlist1 = docIDs1[1]\n",
        "    wordlist2 = docIDs2[1]\n",
        "    m = docIDs1[0]\n",
        "    n = docIDs2[0]\n",
        "    wordlist1.sort()\n",
        "    wordlist2.sort()\n",
        "    p1 = 0\n",
        "    p2 = 0\n",
        "    comparisons = 0\n",
        "    docIDs = []\n",
        "    while p1<m and p2<n:\n",
        "        comparisons += 1\n",
        "        if wordlist1[p1] == wordlist2[p2]:\n",
        "            if p2 == 0:\n",
        "                for i in range(1,wordlist2[p2]+1):\n",
        "                    docIDs.append(i)\n",
        "            else:\n",
        "                for i in range(wordlist2[p2-1]+1,wordlist2[p2]+1):\n",
        "                    docIDs.append(i)\n",
        "            docIDs.append(wordlist1[p1])\n",
        "            p1 += 1\n",
        "            p2 += 1\n",
        "        else:\n",
        "            if wordlist1[p1]>wordlist2[p2]:\n",
        "                if p2 == 0:\n",
        "                    for i in range(1,wordlist2[p2]):\n",
        "                        docIDs.append(i)\n",
        "                else:\n",
        "                    for i in range(wordlist2[p2-1]+1,wordlist2[ptr2]):\n",
        "                        docIDs.append(i)\n",
        "                p2 += 1\n",
        "            else:\n",
        "                p1 += 1 \n",
        "                \n",
        "    while (p2<n):\n",
        "        for i in range(wordlist2[p2-1]+1,wordlist2[p2]):\n",
        "            docIDs.append(i)     \n",
        "            p2 += 1\n",
        "    for i in range(wordlist2[p2-1]+1,len(final_data)+1):\n",
        "        docIDs.append(i)   \n",
        "\n",
        "    return (docIDs, comparisons)"
      ],
      "metadata": {
        "id": "BWpNocT8QjZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_queries(sequence, operations):\n",
        "  comparisons = 0\n",
        "  words = []\n",
        "  for word in sequence:\n",
        "    if(word in unigram_encodings.keys()):\n",
        "      words.append(unigram_encodings[word])\n",
        "    else:\n",
        "      words.append([0,[]])\n",
        "    \n",
        "  prev = words[0]\n",
        "\n",
        "  for i in range(1,len(words)):\n",
        "    l1 = prev\n",
        "    l2 = words[i]\n",
        "    operation = operations[i-1]\n",
        "    if operation == \"OR\":\n",
        "      wor,count = xORy(l1, l2)\n",
        "    elif operation == \"AND\":\n",
        "      wor,count = AND(l1, l2)\n",
        "    elif operation == \"AND NOT\":\n",
        "      wor,count = AND_not(l1, l2)\n",
        "    elif operation == \"OR NOT\":\n",
        "      wor,count = xOR_NOTy(l1, l2)\n",
        "    comparisons += count\n",
        "\n",
        "    prev = [len(wor) , wor]\n",
        " \n",
        "  return prev, comparisons\n"
      ],
      "metadata": {
        "id": "Bf6sPW-LQ21x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start():\n",
        "  \n",
        "  n = int(input(\"number of queries: \"))\n",
        "  for i in range(n):\n",
        "    solutions = []\n",
        "    sequence = input(\"enter sequence: \")\n",
        "    sequence = pre_process_data([sequence])\n",
        "    print(sequence)\n",
        "    operations = input(\"enter operations (comma-separated): \").split(',')\n",
        "    solutions.append(run_queries(sequence[0], operations)) \n",
        "    query = sequence[0][0]\n",
        "    for j in range(len(sequence[0])-1):\n",
        "      query += \" \" + operations[j] + \" \" + sequence[0][j+1]\n",
        "\n",
        "    print(\"Query \" + str(i+1) + \": \" + query)\n",
        "    print(\"Number of documents retrieved for query \" + str(i+1) + \": \" + str(solutions[0][0][0]))\n",
        "    docs = \"\"\n",
        "    for j in range(solutions[0][0][0]):\n",
        "      stri = \"cranfield\"\n",
        "      temp = str(solutions[0][0][1][j])\n",
        "      leng = len(temp)\n",
        "      req = 4 - leng\n",
        "      stri += \"0\" * req \n",
        "      stri += temp\n",
        "      docs += stri\n",
        "      if (j<solutions[0][0][0]-1):\n",
        "        docs += \", \"\n",
        "\n",
        "    print(\"Names of the documents retrieved for query \" + str(i+1) + \": \" + docs)\n",
        "    print(\"Number of comparisons required for query \" + str(i+1) + \": \" +str(solutions[0][1]))"
      ],
      "metadata": {
        "id": "Kzug_b7CLwiF"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start()"
      ],
      "metadata": {
        "id": "uOX0iocYmqTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2403f5-80d7-4aa3-a5a9-4b9d64c7a008"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of queries: 1\n",
            "enter sequence: hello hello\n",
            "[['hello', 'hello']]\n",
            "enter operations (comma-separated): OR\n",
            "Query 1: hello OR hello\n",
            "Number of documents retrieved for query 1: 0\n",
            "Names of the documents retrieved for query 1: \n",
            "Number of comparisons required for query 1: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_encodings[\"incompressible\"]"
      ],
      "metadata": {
        "id": "cjPkVefN2sAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_queries_positional(pre[0])"
      ],
      "metadata": {
        "id": "0YQr6oBUrG9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcTzrT-56L_n",
        "outputId": "2673c921-2e99-400f-fb89-b1671a8f0738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['supersonic', 'aerodynamics']"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre = \"Transient heat rate\"\n",
        "pree = pre_process_data([pre])\n",
        "pree"
      ],
      "metadata": {
        "id": "aD9f12v1G4KD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5545a419-408b-4901-95fd-2d6e4f95b20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['transient', 'heat', 'rate']]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encodings[\"supersonic\"]\n",
        "\n",
        "\n",
        "#  [5, [ {3 : [3, [120, 125, 278]]}, {5 : [1, [28] ] }, {10 : [2, [132, 182]]}, {23 : [3, [0, 12, 28]]}, {27 : [1, [2]]} ] }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j-Up58q8Hj5",
        "outputId": "ba2982f0-7406-4b95-eb21-1b56f5aa0451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[228,\n",
              " {6: [2, [7, 12]],\n",
              "  10: [1, [6]],\n",
              "  13: [1, [145]],\n",
              "  18: [1, [16]],\n",
              "  30: [2, [2, 11]],\n",
              "  32: [1, [72]],\n",
              "  35: [1, [0]],\n",
              "  37: [1, [14]],\n",
              "  38: [1, [21]],\n",
              "  39: [2, [9, 23]],\n",
              "  40: [3, [3, 22, 32]],\n",
              "  47: [1, [0]],\n",
              "  48: [1, [81]],\n",
              "  50: [1, [24]],\n",
              "  51: [1, [15]],\n",
              "  59: [1, [8]],\n",
              "  64: [1, [42]],\n",
              "  73: [1, [26]],\n",
              "  79: [3, [11, 59, 82]],\n",
              "  88: [1, [23]],\n",
              "  92: [2, [0, 10]],\n",
              "  94: [1, [5]],\n",
              "  96: [1, [35]],\n",
              "  117: [1, [68]],\n",
              "  121: [1, [9]],\n",
              "  123: [5, [1, 9, 47, 55, 79]],\n",
              "  125: [1, [2]],\n",
              "  126: [2, [0, 12]],\n",
              "  130: [3, [7, 11, 83]],\n",
              "  135: [1, [7]],\n",
              "  145: [2, [0, 122]],\n",
              "  146: [1, [0]],\n",
              "  156: [1, [121]],\n",
              "  160: [1, [0]],\n",
              "  171: [1, [37]],\n",
              "  172: [1, [8]],\n",
              "  173: [2, [1, 69]],\n",
              "  174: [1, [3]],\n",
              "  175: [2, [6, 14]],\n",
              "  176: [1, [8]],\n",
              "  178: [2, [3, 128]],\n",
              "  181: [2, [3, 8]],\n",
              "  185: [2, [2, 11]],\n",
              "  186: [2, [3, 99]],\n",
              "  187: [2, [3, 117]],\n",
              "  188: [1, [5]],\n",
              "  192: [1, [12]],\n",
              "  199: [2, [28, 44]],\n",
              "  200: [1, [0]],\n",
              "  210: [2, [57, 71]],\n",
              "  211: [1, [24]],\n",
              "  212: [3, [1, 21, 32]],\n",
              "  213: [3, [1, 18, 64]],\n",
              "  215: [7, [0, 11, 59, 61, 75, 91, 111]],\n",
              "  220: [3, [3, 9, 13]],\n",
              "  223: [1, [6]],\n",
              "  224: [1, [4]],\n",
              "  226: [2, [17, 43]],\n",
              "  230: [2, [3, 10]],\n",
              "  241: [2, [6, 14]],\n",
              "  242: [2, [4, 9]],\n",
              "  247: [1, [8]],\n",
              "  250: [2, [5, 21]],\n",
              "  252: [2, [5, 22]],\n",
              "  258: [1, [4]],\n",
              "  271: [1, [71]],\n",
              "  272: [1, [26]],\n",
              "  275: [1, [4]],\n",
              "  277: [3, [5, 13, 46]],\n",
              "  278: [1, [0]],\n",
              "  279: [1, [32]],\n",
              "  283: [2, [1, 12]],\n",
              "  287: [2, [27, 89]],\n",
              "  300: [1, [3]],\n",
              "  318: [1, [46]],\n",
              "  327: [1, [16]],\n",
              "  339: [1, [9]],\n",
              "  344: [1, [8]],\n",
              "  345: [2, [6, 9]],\n",
              "  359: [1, [50]],\n",
              "  368: [2, [2, 21]],\n",
              "  370: [2, [3, 22]],\n",
              "  372: [2, [7, 123]],\n",
              "  390: [1, [14]],\n",
              "  394: [1, [111]],\n",
              "  405: [2, [3, 14]],\n",
              "  408: [2, [4, 22]],\n",
              "  409: [3, [0, 8, 22]],\n",
              "  414: [1, [24]],\n",
              "  425: [4, [9, 12, 28, 83]],\n",
              "  426: [3, [97, 102, 109]],\n",
              "  427: [1, [4]],\n",
              "  428: [1, [10]],\n",
              "  429: [1, [10]],\n",
              "  432: [3, [12, 30, 92]],\n",
              "  433: [1, [5]],\n",
              "  438: [1, [40]],\n",
              "  454: [2, [4, 43]],\n",
              "  463: [1, [101]],\n",
              "  465: [1, [14]],\n",
              "  471: [2, [1, 10]],\n",
              "  486: [2, [1, 31]],\n",
              "  510: [2, [8, 71]],\n",
              "  511: [1, [5]],\n",
              "  512: [1, [2]],\n",
              "  518: [2, [2, 5]],\n",
              "  520: [1, [29]],\n",
              "  528: [2, [7, 60]],\n",
              "  560: [2, [8, 62]],\n",
              "  573: [1, [42]],\n",
              "  596: [1, [7]],\n",
              "  607: [2, [2, 9]],\n",
              "  625: [2, [1, 70]],\n",
              "  626: [1, [25]],\n",
              "  627: [1, [14]],\n",
              "  632: [1, [7]],\n",
              "  656: [1, [2]],\n",
              "  657: [1, [36]],\n",
              "  662: [1, [13]],\n",
              "  679: [2, [4, 13]],\n",
              "  680: [2, [8, 60]],\n",
              "  681: [1, [4]],\n",
              "  682: [2, [13, 58]],\n",
              "  684: [2, [98, 110]],\n",
              "  693: [1, [5]],\n",
              "  695: [1, [55]],\n",
              "  707: [1, [6]],\n",
              "  709: [1, [110]],\n",
              "  718: [3, [51, 83, 94]],\n",
              "  746: [1, [7]],\n",
              "  773: [1, [21]],\n",
              "  778: [2, [40, 51]],\n",
              "  789: [1, [8]],\n",
              "  790: [2, [26, 44]],\n",
              "  791: [1, [27]],\n",
              "  797: [4, [9, 172, 176, 307]],\n",
              "  801: [2, [1, 9]],\n",
              "  803: [1, [64]],\n",
              "  805: [1, [34]],\n",
              "  809: [1, [4]],\n",
              "  810: [1, [99]],\n",
              "  813: [2, [3, 19]],\n",
              "  859: [1, [21]],\n",
              "  875: [1, [24]],\n",
              "  879: [2, [2, 10]],\n",
              "  893: [1, [20]],\n",
              "  895: [1, [2]],\n",
              "  902: [1, [18]],\n",
              "  913: [1, [2]],\n",
              "  914: [2, [2, 14]],\n",
              "  916: [1, [95]],\n",
              "  917: [2, [6, 15]],\n",
              "  919: [2, [0, 16]],\n",
              "  920: [2, [30, 66]],\n",
              "  921: [2, [0, 11]],\n",
              "  922: [1, [5]],\n",
              "  923: [3, [21, 94, 99]],\n",
              "  926: [1, [9]],\n",
              "  968: [1, [13]],\n",
              "  969: [2, [43, 47]],\n",
              "  972: [1, [10]],\n",
              "  978: [2, [32, 71]],\n",
              "  986: [1, [18]],\n",
              "  991: [1, [8]],\n",
              "  992: [3, [13, 27, 30]],\n",
              "  993: [1, [6]],\n",
              "  996: [3, [15, 19, 70]],\n",
              "  997: [1, [56]],\n",
              "  1005: [1, [30]],\n",
              "  1006: [1, [13]],\n",
              "  1060: [2, [5, 16]],\n",
              "  1073: [1, [9]],\n",
              "  1074: [2, [4, 38]],\n",
              "  1095: [2, [7, 21]],\n",
              "  1104: [1, [2]],\n",
              "  1107: [2, [2, 12]],\n",
              "  1109: [1, [0]],\n",
              "  1111: [1, [6]],\n",
              "  1150: [2, [1, 20]],\n",
              "  1178: [1, [108]],\n",
              "  1186: [1, [9]],\n",
              "  1190: [1, [10]],\n",
              "  1191: [1, [3]],\n",
              "  1194: [1, [116]],\n",
              "  1195: [1, [4]],\n",
              "  1196: [2, [4, 15]],\n",
              "  1198: [1, [2]],\n",
              "  1201: [2, [9, 22]],\n",
              "  1206: [2, [0, 19]],\n",
              "  1207: [1, [33]],\n",
              "  1209: [1, [64]],\n",
              "  1210: [2, [3, 10]],\n",
              "  1211: [1, [6]],\n",
              "  1221: [2, [6, 35]],\n",
              "  1223: [1, [2]],\n",
              "  1224: [1, [8]],\n",
              "  1227: [2, [5, 19]],\n",
              "  1232: [2, [0, 7]],\n",
              "  1242: [2, [0, 4]],\n",
              "  1246: [1, [0]],\n",
              "  1247: [1, [21]],\n",
              "  1254: [1, [43]],\n",
              "  1257: [1, [9]],\n",
              "  1258: [2, [3, 22]],\n",
              "  1261: [1, [5]],\n",
              "  1265: [1, [17]],\n",
              "  1266: [2, [0, 7]],\n",
              "  1268: [2, [1, 13]],\n",
              "  1269: [2, [0, 63]],\n",
              "  1270: [3, [1, 4, 20]],\n",
              "  1271: [2, [13, 19]],\n",
              "  1279: [1, [5]],\n",
              "  1283: [1, [7]],\n",
              "  1287: [1, [13]],\n",
              "  1299: [2, [7, 125]],\n",
              "  1300: [1, [77]],\n",
              "  1301: [1, [77]],\n",
              "  1319: [1, [39]],\n",
              "  1327: [1, [56]],\n",
              "  1338: [2, [7, 22]],\n",
              "  1342: [3, [1, 45, 87]],\n",
              "  1363: [1, [21]],\n",
              "  1365: [1, [98]],\n",
              "  1366: [1, [16]],\n",
              "  1373: [1, [6]],\n",
              "  1376: [1, [6]],\n",
              "  1379: [1, [5]],\n",
              "  1392: [1, [80]]}]"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def phrase_queries_positional(q):\n",
        "  candidate = []\n",
        "  if len(q) == 1:\n",
        "\n",
        "    if q in list(unigram_encodings.keys()):\n",
        "        # word is present \n",
        "        return unigram_encodings[q[0]][1]\n",
        "    else:\n",
        "        # word not present\n",
        "        print(\"No document found.\")\n",
        "        return []\n",
        "\n",
        "  else:\n",
        "      for idx in positional_encodings[q[0]][1].keys():\n",
        "        #   print(\"check\" ,idx)\n",
        "          # positional_encodings -> {\"hello\" : [5, [ {3 : [3, [120, 125, 278]]}, {5 : [1, [28] ] }, {10 : [2, [132, 182]]}, {23 : [3, [0, 12, 28]]}, {27 : [1, [2]]} ] }\n",
        "          #             [5,  ]\n",
        "          # [ {3 : [3, [120, 125, 278]]}, {5 : [1, [28] ] }, {10 : [2, [132, 182]]}, {23 : [3, [0, 12, 28]]}, {27 : [1, [2]]}]\n",
        "          for idy in positional_encodings[q[1]][1].keys():\n",
        "            #   print(\"hello\" , idx , id/y)\n",
        "              if idx == idy:\n",
        "                  for pos in positional_encodings[q[0]][1][idx][1]:\n",
        "                      if (pos + 1) in positional_encodings[q[1]][1][idx][1]:\n",
        "                          d = []\n",
        "                          e = []\n",
        "                          d.append(idx)\n",
        "                          e.append(pos)\n",
        "                          e.append(pos + 1)\n",
        "                          d.append(e)\n",
        "                          candidate.append(d)\n",
        "  return (candidate,0)"
      ],
      "metadata": {
        "id": "9MuiX0c0sHw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(ans,s) = phrase_queries_positional(pree[0])"
      ],
      "metadata": {
        "id": "vzwUoMdf5l0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPdySM51Qc7F",
        "outputId": "091c6823-5a40-4cd3-e430-dbc9eede2923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, [1, 2]], [4, [14, 15]], [5, [1, 2]], [158, [2, 3]], [977, [9, 10]]]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_helper(text):\n",
        "    \n",
        "    i = 0\n",
        "    j = 1\n",
        "\n",
        "    ans = []\n",
        "    possible_docs = []\n",
        "    store = []\n",
        "\n",
        "    while(j < len(text)):\n",
        "\n",
        "        word1 = text[i]\n",
        "        word2 = text[j]\n",
        "\n",
        "\n",
        "        (info,comparisions) = phrase_queries_positional([word1,word2])\n",
        "\n",
        "        info_dict = {}\n",
        "        for auto in info:\n",
        "            doc_id = auto[0]\n",
        "            if (j == 1):\n",
        "                possible_docs.append(doc_id)\n",
        "            pos = auto[1][0]\n",
        "            \n",
        "            if(doc_id in info_dict.keys()):\n",
        "                info_dict[doc_id].append(pos)\n",
        "            else:\n",
        "                info_dict[doc_id] = [pos]\n",
        "        store.append(info_dict)\n",
        "\n",
        "        j += 1\n",
        "\n",
        "    # Now we have to check the ordering of the query\n",
        "\n",
        "    count = 0\n",
        "\n",
        "\n",
        "    for i in possible_docs:\n",
        "\n",
        "        flag = False\n",
        "\n",
        "        bigrams_info = []\n",
        "\n",
        "        # For every document we will make a list of list\n",
        "\n",
        "        for dic in store:\n",
        "            if i not in dic.keys():\n",
        "                flag = True\n",
        "                break\n",
        "            bigrams_info.append(dic[i])\n",
        "        if(flag):\n",
        "            continue\n",
        "\n",
        "        # Now we have list of positions of bigrams for ith document\n",
        "\n",
        "        check = True\n",
        "\n",
        "        for ii in range(len(bigrams_info)-1):\n",
        "            if max(bigrams_info[ii]) <= max(bigrams_info[ii+1]):\n",
        "                check = False\n",
        "            \n",
        "        if (check):\n",
        "            ans.append(i)\n",
        "\n",
        "    return ans\n"
      ],
      "metadata": {
        "id": "RO3I5mtQNEKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"laminar boundarylayer\"\n",
        "txt = pre_process_data([txt])\n",
        "lis = positional_helper(txt[0])\n",
        "\n",
        "lis"
      ],
      "metadata": {
        "id": "pyKzoYpqVTwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed61d56-9183-4abb-99b2-8556f6b30ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8,\n",
              " 53,\n",
              " 54,\n",
              " 58,\n",
              " 61,\n",
              " 72,\n",
              " 93,\n",
              " 132,\n",
              " 188,\n",
              " 306,\n",
              " 324,\n",
              " 335,\n",
              " 336,\n",
              " 337,\n",
              " 341,\n",
              " 354,\n",
              " 456,\n",
              " 456,\n",
              " 457,\n",
              " 460,\n",
              " 474,\n",
              " 539,\n",
              " 558,\n",
              " 788,\n",
              " 932,\n",
              " 1199,\n",
              " 1239,\n",
              " 1256,\n",
              " 1281,\n",
              " 1324,\n",
              " 1381]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(lis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiBdJp__V0gi",
        "outputId": "7e5e0ea0-d003-4602-9204-181658a536df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(final_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrtNhmVV5s-Y",
        "outputId": "fa47c7a3-b851-4768-d7e1-4cbbff53812d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1400"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data[926]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xQPEClFKO7J",
        "outputId": "2703c88f-f85f-4ecd-870a-355022b34b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['investigation',\n",
              " 'normal',\n",
              " 'force',\n",
              " 'distributions',\n",
              " 'wakevortex',\n",
              " 'characteristics',\n",
              " 'bodies',\n",
              " 'revolution',\n",
              " 'supersonicspeeds',\n",
              " 'supersonic',\n",
              " 'aerodynamic',\n",
              " 'characteristics',\n",
              " 'inclined',\n",
              " 'bodiesof',\n",
              " 'revolution',\n",
              " 'high',\n",
              " 'angles',\n",
              " 'attack',\n",
              " 'investigated',\n",
              " 'inorder',\n",
              " 'provide',\n",
              " 'basic',\n",
              " 'understanding',\n",
              " 'body',\n",
              " 'vortexwake',\n",
              " 'flow',\n",
              " 'relation',\n",
              " 'problem',\n",
              " 'bodywing',\n",
              " 'interference',\n",
              " 'results',\n",
              " 'windtunnel',\n",
              " 'tests',\n",
              " 'whereby',\n",
              " 'normalforce',\n",
              " 'pitching',\n",
              " 'moment',\n",
              " 'normal',\n",
              " 'force',\n",
              " 'distributions',\n",
              " 'localflow',\n",
              " 'properties',\n",
              " 'vicinity',\n",
              " 'body',\n",
              " 'determined',\n",
              " 'arediscussed',\n",
              " 'analyzed',\n",
              " 'comparisons',\n",
              " 'experimental',\n",
              " 'normal',\n",
              " 'force',\n",
              " 'coefficient',\n",
              " 'andcenter',\n",
              " 'pressure',\n",
              " 'data',\n",
              " 'values',\n",
              " 'calculated',\n",
              " 'accordance',\n",
              " 'withtheories',\n",
              " 'include',\n",
              " 'methods',\n",
              " 'estimating',\n",
              " 'effects',\n",
              " 'viscosityshow',\n",
              " 'accuracy',\n",
              " 'estimates',\n",
              " 'strongly',\n",
              " 'dependenton',\n",
              " 'body',\n",
              " 'fineness',\n",
              " 'ratio',\n",
              " 'angle',\n",
              " 'attack',\n",
              " 'further',\n",
              " 'comparisons',\n",
              " 'distributions',\n",
              " 'theoretical',\n",
              " 'experimentallyderived',\n",
              " 'crossflow',\n",
              " 'drag',\n",
              " 'coefficients',\n",
              " 'clearly',\n",
              " 'show',\n",
              " 'thatin',\n",
              " 'general',\n",
              " 'disagreement',\n",
              " 'experiment',\n",
              " 'existingtheories',\n",
              " 'due',\n",
              " 'inadequate',\n",
              " 'prediction',\n",
              " 'magnitude',\n",
              " 'anddistribution',\n",
              " 'forces',\n",
              " 'resulting',\n",
              " 'flow',\n",
              " 'separation',\n",
              " 'circulation',\n",
              " 'strengths',\n",
              " 'concentrated',\n",
              " 'vortices',\n",
              " 'thecirculation',\n",
              " 'strengths',\n",
              " 'vortex',\n",
              " 'feeding',\n",
              " 'sheets',\n",
              " 'bodyvortex',\n",
              " 'wake',\n",
              " 'determined',\n",
              " 'closedcontour',\n",
              " 'velocityperimeterintegrations',\n",
              " 'paths',\n",
              " 'enclosing',\n",
              " 'vortex',\n",
              " 'feeding',\n",
              " 'sheet',\n",
              " 'the',\n",
              " 'values',\n",
              " 'vortex',\n",
              " 'strength',\n",
              " 'calculated',\n",
              " 'manner',\n",
              " 'inclose',\n",
              " 'agreement',\n",
              " 'values',\n",
              " 'predicted',\n",
              " 'vortex',\n",
              " 'strengthformulas',\n",
              " 'written',\n",
              " 'simple',\n",
              " 'theoretical',\n",
              " 'model',\n",
              " 'isassumed',\n",
              " 'crossflow',\n",
              " 'plane',\n",
              " 'along',\n",
              " 'cylindricalportion',\n",
              " 'body',\n",
              " 'represented',\n",
              " 'steady',\n",
              " 'incompressiblepotential',\n",
              " 'flow',\n",
              " 'cylinder',\n",
              " 'two',\n",
              " 'symmetrical',\n",
              " 'vortices',\n",
              " 'ofequal',\n",
              " 'strength',\n",
              " 'attendant',\n",
              " 'image',\n",
              " 'vortices',\n",
              " 'howeverin',\n",
              " 'computing',\n",
              " 'strengths',\n",
              " 'necessary',\n",
              " 'use',\n",
              " 'vortexlocations',\n",
              " 'viscous',\n",
              " 'normal',\n",
              " 'force',\n",
              " 'distributions',\n",
              " 'determinedfrom',\n",
              " 'experiment',\n",
              " 'experimentally',\n",
              " 'determined',\n",
              " 'values',\n",
              " 'vortex',\n",
              " 'strength',\n",
              " 'arein',\n",
              " 'turn',\n",
              " 'used',\n",
              " 'calculateby',\n",
              " 'means',\n",
              " 'aforementioned',\n",
              " 'incompressiblecrossflow',\n",
              " 'potentialthe',\n",
              " 'local',\n",
              " 'flow',\n",
              " 'inclinationangles',\n",
              " 'good',\n",
              " 'agreement',\n",
              " 'measured',\n",
              " 'valuesexcept',\n",
              " 'vortex',\n",
              " 'core',\n",
              " 'vicinity',\n",
              " 'feeding',\n",
              " 'sheetand',\n",
              " 'regions',\n",
              " 'transonic',\n",
              " 'crossflow',\n",
              " 'velocities',\n",
              " 'expected',\n",
              " 'consideration',\n",
              " 'various',\n",
              " 'regions',\n",
              " 'simplemethods',\n",
              " 'account',\n",
              " 'observed',\n",
              " 'phenomena',\n",
              " 'leads',\n",
              " 'tosubstantial',\n",
              " 'improvement',\n",
              " 'agreement',\n",
              " 'theory',\n",
              " 'andexperiment',\n",
              " 'indicated',\n",
              " 'complete',\n",
              " 'vortex',\n",
              " 'wake',\n",
              " 'flow',\n",
              " 'may',\n",
              " 'adequatelypredicted',\n",
              " 'body',\n",
              " 'revolution',\n",
              " 'for',\n",
              " 'conditions',\n",
              " 'representedby',\n",
              " 'theoretical',\n",
              " 'flow',\n",
              " 'model',\n",
              " 'provided',\n",
              " 'distributionof',\n",
              " 'viscous',\n",
              " 'normal',\n",
              " 'force',\n",
              " 'vortex',\n",
              " 'locations',\n",
              " 'areaccurately',\n",
              " 'known']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def phrase_queries_bigram(query):\n",
        "  meta_docIDs = []\n",
        "  docIDs = []\n",
        "  for i in range(len(query)-1):\n",
        "    search_bigram = query[i]+query[i+1]\n",
        "    if search_bigram in bigram_encodings.keys():\n",
        "      tempIDs = bigram_encodings[search_bigram][1]\n",
        "      meta_docIDs.append(tempIDs)\n",
        "  if len(meta_docIDs) == 1:\n",
        "      return meta_docIDs[0]\n",
        "\n",
        "  docIDs = meta_docIds[0]\n",
        "  for i in range(len(meta_docIDs)):\n",
        "    docIDs = [IDs for IDs in meta_docIDs[i] if IDs in docIDs]\n",
        "  return set(docIDs)"
      ],
      "metadata": {
        "id": "s0eg9Nq7KddS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"laminar boundarylayer\"\n",
        "txt = pre_process_data([txt])\n",
        "lis = phrase_queries_bigram(txt[0])\n",
        "lis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYURcaZ6Yt_Q",
        "outputId": "6b879b74-b5c5-4237-dc2e-a47aa3cf3538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3,\n",
              " 8,\n",
              " 49,\n",
              " 53,\n",
              " 54,\n",
              " 58,\n",
              " 58,\n",
              " 61,\n",
              " 72,\n",
              " 93,\n",
              " 132,\n",
              " 144,\n",
              " 188,\n",
              " 304,\n",
              " 306,\n",
              " 324,\n",
              " 325,\n",
              " 335,\n",
              " 336,\n",
              " 337,\n",
              " 341,\n",
              " 354,\n",
              " 456,\n",
              " 456,\n",
              " 457,\n",
              " 457,\n",
              " 460,\n",
              " 474,\n",
              " 539,\n",
              " 558,\n",
              " 561,\n",
              " 610,\n",
              " 661,\n",
              " 709,\n",
              " 788,\n",
              " 932,\n",
              " 1184,\n",
              " 1191,\n",
              " 1198,\n",
              " 1199,\n",
              " 1239,\n",
              " 1256,\n",
              " 1277,\n",
              " 1281,\n",
              " 1299,\n",
              " 1322,\n",
              " 1324,\n",
              " 1346,\n",
              " 1380,\n",
              " 1381,\n",
              " 1381]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6SlLrDBbwYV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}